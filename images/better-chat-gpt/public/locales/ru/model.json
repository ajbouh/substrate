{
  "configuration": "Конфигурация",
  "model": "Модель",
  "token": {
    "label": "Макс. токенов",
    "description": "Максимальное количество токенов для генерации в чате. Общая длина входных токенов и сгенерированных токенов ограничена контекстной длиной модели."
  },
  "default": "По умолчанию",
  "temperature": {
    "label": "Температура",
    "description": "Значение температуры выборки от 0 до 2. Более высокие значения, например 0.8, сделают выходной результат более случайным, в то время как более низкие значения, например 0.2, более фокусированными и детерминированными. Мы обычно рекомендуем изменять это или top-p, но не оба. (По умолчанию: 1)"
  },
  "presencePenalty": {
    "label": "Штраф за присутствие",
    "description": "Число от -2.0 до 2.0. Положительные значения штрафуют новые токены на основе их появления в тексте до этого момента, увеличивая вероятность перехода модели к новым темам. (По умолчанию: 0)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Число от 0 до 1. Альтернатива выборке с температурой, называемая выборка ядра, при которой модель учитывает результаты токенов с верхним p вероятностных масс. Так, значение 0.1 означает, что рассматриваются только токены, составляющие верхние 10% вероятностной массы. Мы обычно рекомендуем изменять это или температуру, но не оба. (По умолчанию: 1)"
  },
  "frequencyPenalty": {
    "label": "Штраф за частоту",
    "description": "Число от -2.0 до 2.0. Положительные значения штрафуют новые токены на основе их имеющейся частоты в тексте на данный момент, уменьшая вероятность повторения той же строки дословно. (По умолчанию: 0)"
  },
  "defaultChatConfig": "Конфигурация чата по умолчанию",
  "defaultSystemMessage": "Системное сообщение по умолчанию",
  "resetToDefault": "Восстановить значения по умолчанию"
}
