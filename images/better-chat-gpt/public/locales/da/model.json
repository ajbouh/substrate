{
  "configuration": "Konfiguration",
  "model": "Model",
  "token": {
    "label": "Max Token",
    "description": "Det maksimale antal tokens der skal genereres i chat-fuldførelsen. Den samlede længde af input tokens og genererede tokens er begrænset af modellens kontekstlængde."
  },
  "default": "Standard",
  "temperature": {
    "label": "Temperatur",
    "description": "Hvilken samplingstemperatur der skal bruges, mellem 0 og 2. Højere værdier som 0,8 vil gøre outputtet mere tilfældigt, mens lavere værdier som 0,2 vil gøre det mere fokuseret og deterministisk. Vi anbefaler generelt at ændre dette eller top p, men ikke begge. (Standard: 1)"
  },
  "presencePenalty": {
    "label": "Tilstedeværelsesstraf",
    "description": "Tal mellem -2,0 og 2,0. Positive værdier straffer nye tokens baseret på, om de vises i teksten hidtil, hvilket øger modellens sandsynlighed for at tale om nye emner. (Standard: 0)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Tal mellem 0 og 1. Et alternativ til prøvetagning med temperatur, kaldet nucleus sampling, hvor modellen overvejer resultaterne af tokens med top p sandsynlighedsmasse. Så 0,1 betyder, at kun tokens, der udgør de øverste 10% sandsynlighedsmasse, overvejes. Vi anbefaler generelt at ændre dette eller temperaturen, men ikke begge. (Standard: 1)"
  },
  "frequencyPenalty": {
    "label": "Frekvensstraf",
    "description": "Tal mellem -2,0 og 2,0. Positive værdier straffer nye tokens baseret på deres eksisterende frekvens i teksten hidtil, hvilket nedsætter modellens sandsynlighed for at gentage den samme linje ordret. (Standard: 0)"
  },
  "defaultChatConfig": "Standard Chat-konfiguration",
  "defaultSystemMessage": "Standard Systembesked",
  "resetToDefault": "Nulstil til standard"
}
