<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8"> 
        <script src="./ohm.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/markdown-it@14.1.0/dist/markdown-it.js"></script>
        <link rel="stylesheet" href="./msg.css">
    </head>
    <body>
        <div id="renkon">
            <script type="reactive">
                const {h, render, html} = import(Renkon.spaceURL("./preact.standalone.module.js"));
                const {dom: msgDOM} = import("./msg.js")
                const jsonpointer = import("./jsonpointer.js")
                const md = (() => {
                    return window.markdownit();
                })()

                console.log({jsonpointer})
            
                // Renkon.merge(msg)
                // render(msgDOM, document.querySelector("#msg"))
                
                const msg2 = JSON.parse(`{"name":"watt-tool-8b:completion","msg":{"data":{"parameters":{"model":"model","stream":false}},"meta":{"#/data/parameters/cache_prompt":{"description":"Save the prompt and generation for avoid reprocess entire prompt if a part of this isn't change (default: false)","type":"bool"},"#/data/parameters/frequency_penalty":{"description":"Repeat alpha frequency penalty (default: 0.0, 0.0 = disabled);","type":"number"},"#/data/parameters/grammar":{"description":"Set grammar for grammar-based sampling (default: no grammar)","type":"string"},"#/data/parameters/ignore_eos":{"description":"Ignore end of stream token and continue generating (default: false).","type":"bool"},"#/data/parameters/image_data":{"description":"An array of objects to hold base64-encoded image data and its ids to be reference in prompt. You can determine the place of the image in the prompt as in the following: USER:[img-12]Describe the image in detail.\\\\nASSISTANT:. In this case, [img-12] will be replaced by the embeddings of the image with id 12 in the following image_data array: {..., \\"image_data\\": [{\\"data\\": \\"<BASE64_STRING>\\", \\"id\\": 12}]}. Use image_data only with multimodal models, e.g., LLaVA.","type":"[]{\\"image_data\\": [{\\"data\\": \\"<BASE64_STRING>\\", \\"id\\": 12}]}"},"#/data/parameters/image_data/*/data":{"description":"Base64-encoded image data","type":"string"},"#/data/parameters/image_data/*/id":{"description":"image id to be referenced in the prompt","type":"number"},"#/data/parameters/logit_bias":{"description":"Modify the likelihood of a token appearing in the generated text completion. For example, use \\"logit_bias\\": [[15043,1.0]] to increase the likelihood of the token 'Hello', or \\"logit_bias\\": [[15043,-1.0]] to decrease its likelihood. Setting the value to false, \\"logit_bias\\": [[15043,false]] ensures that the token Hello is never produced (default: []).","type":"array"},"#/data/parameters/logit_bias/*":{"type":"tuple"},"#/data/parameters/logit_bias/*/0":{"description":"token id to modify the likelihood of","type":"number"},"#/data/parameters/logit_bias/*/1":{"description":"likelihood for the given token. For example, use 1.0 to increase the likelihood of the token, or -1.0 to decrease its likelihood. Setting the value to false ensures that the token is never produced.","type":"number"},"#/data/parameters/min_p":{"description":"The minimum probability for a token to be considered, relative to the probability of the most likely token (default: 0.05).","type":"number"},"#/data/parameters/mirostat":{"description":"Enable Mirostat sampling, controlling perplexity during text generation (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0).","type":"number"},"#/data/parameters/mirostat_eta":{"description":"Set the Mirostat learning rate, parameter eta (default: 0.1).","type":"number"},"#/data/parameters/mirostat_tau":{"description":"Set the Mirostat target entropy, parameter tau (default: 5.0).","type":"number"},"#/data/parameters/model":{"type":"string"},"#/data/parameters/n_keep":{"description":"Specify the number of tokens from the prompt to retain when the context size is exceeded and tokens need to be discarded. By default, this value is set to 0 (meaning no tokens are kept). Use -1 to retain all tokens from the prompt.","type":"number"},"#/data/parameters/n_predict":{"description":"Set the maximum number of tokens to predict when generating text. Note: May exceed the set limit slightly if the last token is a partial multibyte character. When 0, no tokens will be generated but the prompt is evaluated into the cache. (default: -1, -1 = infinity).","type":"number"},"#/data/parameters/n_probs":{"description":"If greater than 0, the response also contains the probabilities of top N tokens for each generated token (default: 0)","type":"number"},"#/data/parameters/penalize_nl":{"description":"Penalize newline tokens when applying the repeat penalty (default: true).","type":"bool"},"#/data/parameters/penalty_prompt":{"description":"This will replace the prompt for the purpose of the penalty evaluation. Can be either null, a string or an array of numbers representing tokens (default: null = use the original prompt).","type":"null | string | []number"},"#/data/parameters/presence_penalty":{"description":"Repeat alpha presence penalty (default: 0.0, 0.0 = disabled).","type":"number"},"#/data/parameters/prompt":{"description":"Provide the prompt for this completion as a string or as an array of strings or numbers representing tokens. Internally, the prompt is compared to the previous completion and only the \\"unseen\\" suffix is evaluated. If the prompt is a string or an array with the first element given as a string, a bos token is inserted in the front like main does.","type":"string"},"#/data/parameters/repeat_last_n":{"description":"Last n tokens to consider for penalizing repetition (default: 64, 0 = disabled, -1 = ctx-size).","type":"number"},"#/data/parameters/repeat_penalty":{"description":"Control the repetition of token sequences in the generated text (default: 1.1).","type":"number"},"#/data/parameters/seed":{"description":"Set the random number generator (RNG) seed (default: -1, -1 = random seed).","type":"number"},"#/data/parameters/slot_id":{"description":"Assign the completion task to an specific slot. If is -1 the task will be assigned to a Idle slot (default: -1)","type":"number"},"#/data/parameters/stop":{"description":"Specify a JSON array of stopping strings. These words will not be included in the completion, so make sure to add them to the prompt for the next iteration (default: []).","type":"[]string"},"#/data/parameters/stream":{"description":"It allows receiving each predicted token in real-time instead of waiting for the completion to finish. To enable this, set to true.","type":"bool"},"#/data/parameters/system_prompt":{"description":"Change the system prompt (initial prompt of all slots), this is useful for chat applications. See more","type":"string"},"#/data/parameters/temperature":{"description":"Adjust the randomness of the generated text (default: 0.8).","type":"number"},"#/data/parameters/tfs_z":{"description":"Enable tail free sampling with parameter z (default: 1.0, 1.0 = disabled).","type":"number"},"#/data/parameters/top_k":{"description":"Limit the next token selection to the K most probable tokens (default: 40).","type":"number"},"#/data/parameters/top_p":{"description":"Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P (default: 0.95).","type":"number"},"#/data/parameters/typical_p":{"description":"Enable locally typical sampling with parameter p (default: 1.0, 1.0 = disabled).","type":"number"},"#/data/returns/content":{"description":"Completion result as a string (excluding stopping_word if any). In case of streaming mode, will contain the next token as a string."},"#/data/returns/generation_settings":{"description":"The provided options above excluding prompt but including n_ctx, model"},"#/data/returns/model":{"description":"The path to the model loaded with -m"},"#/data/returns/prompt":{"description":"The provided prompt"},"#/data/returns/stop":{"description":"Boolean for use with stream to check whether the generation has stopped (Note: This is not related to stopping words array stop from input options)"},"#/data/returns/stopped_eos":{"description":"Indicating whether the completion has stopped because it encountered the EOS token"},"#/data/returns/stopped_limit":{"description":"Indicating whether the completion stopped because n_predict tokens were generated before stop words or EOS was encountered"},"#/data/returns/stopped_word":{"description":"Indicating whether the completion stopped due to encountering a stopping word from stop JSON array provided"},"#/data/returns/stopping_word":{"description":"The stopping word encountered which stopped the generation (or \\"\\" if not stopped due to a stopping word)"},"#/data/returns/timings":{"description":"Hash of timing information about the completion such as the number of tokens predicted_per_second"},"#/data/returns/tokens_cached":{"description":"Number of tokens from the prompt which could be re-used from previous completion (n_past)"},"#/data/returns/tokens_evaluated":{"description":"Number of tokens evaluated in total from the prompt"},"#/data/returns/truncated":{"description":"Boolean indicating if the context size was exceeded during generation, i.e. the number of tokens provided in the prompt (tokens_evaluated) plus tokens generated (tokens predicted) exceeded the context size (n_ctx)"}},"msg":{"cap":"http","data":{"request":{"body":{},"headers":{"Accept":["application/json"],"Content-Type":["application/json"]},"method":"POST","url":"https://substrate-b95b.local/watt-tool-8b/v1/completions"},"response":{}}},"msg_in":{"#/msg/data/request/body/cache_prompt":"#/data/parameters/cache_prompt","#/msg/data/request/body/frequency_penalty":"#/data/parameters/frequency_penalty","#/msg/data/request/body/grammar":"#/data/parameters/grammar","#/msg/data/request/body/ignore_eos":"#/data/parameters/ignore_eos","#/msg/data/request/body/image_data":"#/data/parameters/image_data","#/msg/data/request/body/image_data/*/data":"#/data/parameters/image_data/*/data","#/msg/data/request/body/image_data/*/id":"#/data/parameters/image_data/*/id","#/msg/data/request/body/logit_bias":"#/data/parameters/logit_bias","#/msg/data/request/body/logit_bias/*":"#/data/parameters/logit_bias/*","#/msg/data/request/body/logit_bias/*/0":"#/data/parameters/logit_bias/*/0","#/msg/data/request/body/logit_bias/*/1":"#/data/parameters/logit_bias/*/1","#/msg/data/request/body/min_p":"#/data/parameters/min_p","#/msg/data/request/body/mirostat":"#/data/parameters/mirostat","#/msg/data/request/body/mirostat_eta":"#/data/parameters/mirostat_eta","#/msg/data/request/body/mirostat_tau":"#/data/parameters/mirostat_tau","#/msg/data/request/body/model":"#/data/parameters/model","#/msg/data/request/body/n_keep":"#/data/parameters/n_keep","#/msg/data/request/body/n_predict":"#/data/parameters/n_predict","#/msg/data/request/body/n_probs":"#/data/parameters/n_probs","#/msg/data/request/body/penalize_nl":"#/data/parameters/penalize_nl","#/msg/data/request/body/penalty_prompt":"#/data/parameters/penalty_prompt","#/msg/data/request/body/presence_penalty":"#/data/parameters/presence_penalty","#/msg/data/request/body/prompt":"#/data/parameters/prompt","#/msg/data/request/body/repeat_last_n":"#/data/parameters/repeat_last_n","#/msg/data/request/body/repeat_penalty":"#/data/parameters/repeat_penalty","#/msg/data/request/body/seed":"#/data/parameters/seed","#/msg/data/request/body/slot_id":"#/data/parameters/slot_id","#/msg/data/request/body/stop":"#/data/parameters/stop","#/msg/data/request/body/stream":"#/data/parameters/stream","#/msg/data/request/body/system_prompt":"#/data/parameters/system_prompt","#/msg/data/request/body/temperature":"#/data/parameters/temperature","#/msg/data/request/body/tfs_z":"#/data/parameters/tfs_z","#/msg/data/request/body/top_k":"#/data/parameters/top_k","#/msg/data/request/body/top_p":"#/data/parameters/top_p","#/msg/data/request/body/typical_p":"#/data/parameters/typical_p"},"msg_out":{"#/data/returns/content":"#/msg/data/response/body/content","#/data/returns/generation_settings":"#/msg/data/response/body/generation_settings","#/data/returns/model":"#/msg/data/response/body/model","#/data/returns/prompt":"#/msg/data/response/body/prompt","#/data/returns/stop":"#/msg/data/response/body/stop","#/data/returns/stopped_eos":"#/msg/data/response/body/stopped_eos","#/data/returns/stopped_limit":"#/msg/data/response/body/stopped_limit","#/data/returns/stopped_word":"#/msg/data/response/body/stopped_word","#/data/returns/stopping_word":"#/msg/data/response/body/stopping_word","#/data/returns/timings":"#/msg/data/response/body/timings","#/data/returns/tokens_cached":"#/msg/data/response/body/tokens_cached","#/data/returns/tokens_evaluated":"#/msg/data/response/body/tokens_evaluated","#/data/returns/truncated":"#/msg/data/response/body/truncated"}},"basehref":"https://substrate-b95b.local/"}`)
                const msg3 = {
                    ...msg2,
                    msg: {
                        ...msg2.msg,
                        description: `this is a description

it has markdown:    

- with a list
- in it
                        `,
                    },
                }
                render(msgDOM({h, md, jsonpointer, msg: msg3}), document.querySelector("#msg"))
            </script>
            <div id="msg"></div>
            <style>
            </style>
        </div>
        <script type="module">
            import {view, newInspector} from "./renkon.js";
            view({app: {newInspector}});
        </script>
    </body>
</html>
