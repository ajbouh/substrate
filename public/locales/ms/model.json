{
  "configuration": "Konfigurasi",
  "model": "Model",
  "token": {
    "label": "Token Maksimum",
    "description": "Jumlah token maksimum untuk dijana dalam lengkapan sembang. Panjang keseluruhan token input dan token yang dijana adalah terhad oleh panjang konteks model."
  },
  "default": "Lalai",
  "temperature": {
    "label": "Suhu",
    "description": "Suhu pensampelan yang digunakan, antara 0 dan 2. Nilai yang lebih tinggi seperti 0.8 akan menjadikan keluaran lebih rawak, manakala nilai yang lebih rendah seperti 0.2 akan menjadikannya lebih terarah dan deterministik. Kami secara umumnya mengesyorkan mengubah ini atau atas p tetapi bukan kedua-duanya. (Lalai: 1)"
  },
  "presencePenalty": {
    "label": "Hukuman Kehadiran",
    "description": "Nombor antara -2.0 dan 2.0. Nilai positif menghukum token baru berdasarkan sama ada mereka muncul dalam teks sejauh ini, meningkatkan kemungkinan model untuk bercakap mengenai topik baru. (Lalai: 0)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Nombor antara 0 dan 1. Alternatif kepada pensampelan dengan suhu, dipanggil pensampelan nukleus, di mana model mempertimbangkan hasil token dengan jisim kebarangkalian top p. Jadi 0.1 bermaksud hanya token yang terdiri daripada 10% jisim kebarangkalian teratas dipertimbangkan. Secara umumnya, kami mengesyorkan untuk mengubah suhu atau nilai paling atas tetapi bukan kedua-duanya. (Lalai: 1)"
  },
  "frequencyPenalty": {
    "label": "Penalti Frekuensi",
    "description": "Nombor antara -2.0 dan 2.0. Nilai positif memberi hukuman kepada token baru berdasarkan frekuensi sedia ada mereka dalam teks sejauh ini, mengurangkan kebarangkalian model untuk mengulangi baris yang sama secara harfiah. (Lalai: 0)"
  },
  "defaultChatConfig": "Konfigurasi Cakap Lalai",
  "defaultSystemMessage": "Mesej Sistem Lalai",
  "resetToDefault": "Set Semula ke Lalai"
}
