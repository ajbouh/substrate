{
  "configuration": "Configuration",
  "model": "Modèle",
  "token": {
    "label": "Max Token",
    "description": "Le nombre maximum de jetons à générer dans la complétion de la conversation. La longueur totale des jetons d'entrée et des jetons générés est limitée par la longueur de contexte du modèle."
  },
  "default": "Défaut",
  "temperature": {
    "label": "Température",
    "description": "La température d'échantillonnage, entre 0 et 2. Des valeurs plus élevées comme 0,8 rendent la sortie plus aléatoire, tandis que des valeurs plus basses comme 0,2 la rendent plus ciblée et déterminée. Nous recommandons généralement de modifier ceci ou top-p mais pas les deux. (Par défaut : 1)"
  },
  "presencePenalty": {
    "label": "Pénalité de présence",
    "description": "Nombre entre -2.0 et 2.0. Les valeurs positives pénalisent les nouveaux jetons en fonction de leur apparition dans le texte jusqu'à présent, augmentant la probabilité du modèle de parler de nouveaux sujets. (Par défaut : 0)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Nombre entre 0 et 1. Une alternative à l'échantillonnage avec la température, appelée échantillonnage de noyau, où le modèle considère les résultats des jetons avec une probabilité de p-masse supérieure. Ainsi, 0,1 signifie que seuls les jetons constituant les 10 % supérieurs de la masse de probabilité sont considérés. Nous recommandons généralement de modifier ceci ou la température mais pas les deux. (Par défaut : 1)"
  },
  "frequencyPenalty": {
    "label": "Pénalité de fréquence",
    "description": "Nombre entre -2.0 et 2.0. Les valeurs positives pénalisent les nouveaux jetons en fonction de leur fréquence existante dans le texte jusqu'à présent, diminuant la probabilité du modèle de répéter la même ligne mot pour mot. (Par défaut : 0)"
  },
  "defaultChatConfig": "Configuration de Chat Par Défaut",
  "defaultSystemMessage": "Message Système Par Défaut",
  "resetToDefault": "Réinitialiser aux paramètres par défaut"
}
